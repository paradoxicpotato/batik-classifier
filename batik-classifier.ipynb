{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/isaacbernadus/tubes?scriptVersionId=138126890\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\nimport pathlib\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import regularizers\nBATCH_SIZE = 32\nIMG_HEIGHT = 512\nIMG_WIDTH = 512\n\ndata_dir = \"../input/d/isaacbernadus/batik-data-set/training_files_cleaned\"\ndata_dir = pathlib.Path(data_dir)\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  color_mode=\"grayscale\",\n  shuffle=True,\n  image_size=(IMG_HEIGHT, IMG_WIDTH),\n  batch_size=BATCH_SIZE,\n  crop_to_aspect_ratio=True)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  color_mode=\"grayscale\",\n  image_size=(IMG_HEIGHT, IMG_WIDTH),\n  batch_size=BATCH_SIZE,\n  crop_to_aspect_ratio=True)\n\n\nclass_names = train_ds.class_names\nprint(class_names)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:50:22.510606Z","iopub.execute_input":"2021-12-17T14:50:22.510983Z","iopub.status.idle":"2021-12-17T14:50:22.867123Z","shell.execute_reply.started":"2021-12-17T14:50:22.510927Z","shell.execute_reply":"2021-12-17T14:50:22.866046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for any files that is incompatible with TensorFlow","metadata":{}},{"cell_type":"code","source":"import pathlib\nfrom pathlib import Path\nimport imghdr\n\nimage_extensions = [\".png\",\".jpg\"]  # add there all your images file extensions\nimg_type_accepted_by_tf = [\"jpeg\",\"jpg\"]\nfor filepath in Path(data_dir).rglob(\"*\"):\n    if filepath.suffix.lower() in image_extensions:\n        img_type = imghdr.what(filepath)\n        if img_type is None:\n            print(f\"{filepath} is not an image\")\n        elif img_type not in img_type_accepted_by_tf:\n            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:50:22.8693Z","iopub.execute_input":"2021-12-17T14:50:22.869624Z","iopub.status.idle":"2021-12-17T14:50:23.3474Z","shell.execute_reply.started":"2021-12-17T14:50:22.869578Z","shell.execute_reply":"2021-12-17T14:50:23.346408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune and build model","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\nnum_classes = len(class_names)\n\ndata_augmentation = keras.Sequential(\n  [\n    layers.RandomFlip(\"horizontal\",\n                      input_shape=(IMG_HEIGHT,\n                                  IMG_WIDTH,\n                                  1)),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n  ]\n)\n\nmodel = Sequential([\n  data_augmentation,\n  layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),\n  layers.Conv2D(8, 3, padding='same', activation='relu'),\n  layers.Dropout(0.2),\n  layers.MaxPooling2D(),\n\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.Dropout(0.2),\n  layers.MaxPooling2D(),\n\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.Dropout(0.2),\n  layers.MaxPooling2D(),\n\n  layers.Flatten(),\n  layers.Dropout(0.5),\n  layers.Dense(64, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(num_classes, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# Train Model","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:50:23.350433Z","iopub.execute_input":"2021-12-17T14:50:23.35111Z","iopub.status.idle":"2021-12-17T14:50:23.77596Z","shell.execute_reply.started":"2021-12-17T14:50:23.351062Z","shell.execute_reply":"2021-12-17T14:50:23.775019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=130\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:50:23.778765Z","iopub.execute_input":"2021-12-17T14:50:23.779067Z","iopub.status.idle":"2021-12-17T14:55:04.268921Z","shell.execute_reply.started":"2021-12-17T14:50:23.779019Z","shell.execute_reply":"2021-12-17T14:55:04.267915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:55:04.270653Z","iopub.execute_input":"2021-12-17T14:55:04.270906Z","iopub.status.idle":"2021-12-17T14:55:04.681184Z","shell.execute_reply.started":"2021-12-17T14:55:04.270871Z","shell.execute_reply":"2021-12-17T14:55:04.680254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Model with test_files","metadata":{}},{"cell_type":"code","source":"test_directory = \"../input/d/isaacbernadus/batik-data-set/test_files\"\nfiles = pathlib.Path(test_directory).glob('*')\n\nprint (\"{:<50} {:<20} {:<30} {:<20} {:<20}\".format('File','Kawung','Mega Mendung','Parang', 'Truntum'))\nfor file in files:\n#     print(os.path.basename(file))\n    img = tf.keras.utils.load_img(\n    file,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    color_mode=\"grayscale\"\n    )\n    img_array = tf.keras.utils.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0) # Create a batch\n\n    predictions = model.predict(img_array)\n    score = tf.nn.softmax(predictions[0])\n    print (\"{:<50} {:<20} {:<30} {:<20} {:<20}\".format(os.path.basename(file), \n                                 str(round(predictions[0,0]*100,2)), \n                                 str(round(predictions[0,1]*100,2)),\n                                 str(round(predictions[0,2]*100,2)),\n                                 str(round(predictions[0,3]*100,2))))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:55:04.682772Z","iopub.execute_input":"2021-12-17T14:55:04.683517Z","iopub.status.idle":"2021-12-17T14:55:10.595906Z","shell.execute_reply.started":"2021-12-17T14:55:04.683457Z","shell.execute_reply":"2021-12-17T14:55:10.594879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p saved_model\nmodel.save('saved_model/my_model')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:55:10.597519Z","iopub.execute_input":"2021-12-17T14:55:10.598183Z","iopub.status.idle":"2021-12-17T14:55:15.365706Z","shell.execute_reply.started":"2021-12-17T14:55:10.598136Z","shell.execute_reply":"2021-12-17T14:55:15.364624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pathlib\nimport tensorflow as tf\nimport numpy as np\nmodel2 = tf.keras.models.load_model('saved_model/my_model')\n\ntest_directory = \"../input/d/isaacbernadus/batik-data-set/test_files\"\nfiles = pathlib.Path(test_directory).glob('*')\n\nclass_name = [\n    \"Kawung\",\n    \"Mega_Mendung\",\n    \"Parang\",\n    \"Truntum\"\n]\n\nkawung_test = pathlib.Path(test_directory).glob('*kawung*')\ntruntum_test = pathlib.Path(test_directory).glob('*truntum*')\nparang_test = pathlib.Path(test_directory).glob('*parang*')\nmega_mendung_test = pathlib.Path(test_directory).glob('*mega_mendung*')\ntest_sum = [0,0,0,0]\nfor file in kawung_test:\n    test_sum[0] += 1\n\nfor file in mega_mendung_test:\n    test_sum[1] += 1\n    \nfor file in parang_test:\n    test_sum[2] += 1\n\nfor file in truntum_test:\n    test_sum[3] += 1\n    \nval_sum = [0,0,0,0]\nval_sum_true = [0,0,0,0]\nval_sum_true_neg = [0,0,0,0]\nprint(\"PENGENAL CORAK BATIK\")\nprint(\"Sampel: training_files_b\")\nprint(\"Metode: CNN dengan Tensorflow dan Multilabel Classification\")\n\nprint(\"------\")\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10}\".format('File','Kawung','Mega Mendung','Parang', 'Truntum'))\nfor file in files:\n#     print(os.path.basename(file))\n    img = tf.keras.utils.load_img(\n    file,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    color_mode=\"grayscale\"\n    )\n    img_array = tf.keras.utils.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0) # Create a batch\n\n    predictions = model2.predict(img_array)\n    filename = os.path.basename(file)\n    \n    # True Positive\n    if class_name[np.argmax(predictions)] == \"Kawung\" and filename.find(\"kawung\") !=-1 :\n        val_sum_true[0] += 1\n    elif class_name[np.argmax(predictions)] == \"Mega_Mendung\" and filename.find(\"mega_mendung\") !=-1:\n        val_sum_true[1] += 1\n    elif class_name[np.argmax(predictions)] == \"Parang\" and filename.find(\"parang\")!=-1:\n        val_sum_true[2] += 1\n    elif class_name[np.argmax(predictions)] == \"Truntum\" and filename.find(\"truntum\")!=-1:\n        val_sum_true[3] += 1\n        \n    # False Positive\n    if class_name[np.argmax(predictions)] == \"Kawung\" and filename.find(\"kawung\") ==-1 :\n        val_sum[0] += 1\n    elif class_name[np.argmax(predictions)] == \"Mega_Mendung\" and filename.find(\"mega_mendung\") ==-1 :\n        val_sum[1] += 1\n    elif class_name[np.argmax(predictions)] == \"Parang\" and filename.find(\"parang\") == -1 :\n        val_sum[2] += 1\n    elif class_name[np.argmax(predictions)] == \"Truntum\" and filename.find(\"truntum\") == -1:\n        val_sum[3] += 1\n        \n    #False Negative\n    val_sum_false_neg=[0,0,0,0]\n    \n    val_sum_false_neg[0] = test_sum[0] - val_sum_true[0]\n    val_sum_false_neg[1] = test_sum[1] - val_sum_true[1]\n    val_sum_false_neg[2] = test_sum[2] - val_sum_true[2]\n    val_sum_false_neg[3] = test_sum[3] - val_sum_true[3]\n    \n    #True Negative\n    if filename.find(\"kawung\") ==-1 and class_name[np.argmax(predictions)] != \"Kawung\" :\n        val_sum_true_neg[0] += 1\n    elif filename.find(\"mega_mendung\") ==-1 and class_name[np.argmax(predictions)] != \"Mega_Mendung\":\n        val_sum_true_neg[1] += 1\n    elif filename.find(\"parang\") ==-1 and class_name[np.argmax(predictions)] != \"Parang\":\n        val_sum_true_neg[2] += 1\n    elif  filename.find(\"truntum\") ==-1 and class_name[np.argmax(predictions)] != \"Truntum\":\n        val_sum_true_neg[3] += 1\n    \n    print (\"{:<50} {:<10} {:<20} {:<10} {:<10}\".format(os.path.basename(file), \n                                 str(round(predictions[0,0]*100,2)), \n                                 str(round(predictions[0,1]*100,2)),\n                                 str(round(predictions[0,2]*100,2)),\n                                 str(round(predictions[0,3]*100,2))))\n    \n\nprint(\"\")\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10} {:<10}\".format('Kinerja','Kawung','Mega Mendung','Parang', 'Truntum', \"TOTAL\"))\nprint(\"------\")\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10} {:<10}\".format('True-Positif', val_sum_true[0], val_sum_true[1], val_sum_true[2], val_sum_true[3], sum(val_sum_true)))\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10} {:<10}\".format('False-Positif', val_sum[0], val_sum[1], val_sum[2], val_sum[3], sum(val_sum)))\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10} {:<10}\".format('True-Negative', val_sum_true_neg[0], val_sum_true_neg[1], val_sum_true_neg[2], val_sum_true_neg[3], sum(val_sum_true_neg)))\nprint (\"{:<50} {:<10} {:<20} {:<10} {:<10} {:<10}\".format('False-Negative', val_sum_false_neg[0], val_sum_false_neg[1], val_sum_false_neg[2], val_sum_false_neg[3], sum(val_sum_false_neg)))\n\n\naccuracy=(sum(val_sum_true) + sum(val_sum_true_neg))/(sum(val_sum_true) + sum(val_sum_true_neg) + sum(val_sum) + sum(val_sum_false_neg))\nprecision = sum(val_sum_true) / (sum(val_sum_true) + sum(val_sum))\nsensitivity = sum(val_sum_true)/ (sum(val_sum_true) + sum(val_sum_false_neg))\nspecificity = sum(val_sum_true_neg) / (sum(val_sum_true_neg) + sum(val_sum_false_neg))\nf1_score = (2*sensitivity*precision)/(sensitivity + precision)\nprint(\"-----\")\nprint(\"{:<50} {:<20}\".format(\"Accuracy (%)\", accuracy*100))\nprint(\"{:<50} {:<20}\".format(\"Precision (%)\", precision*100))\nprint(\"{:<50} {:<20}\".format(\"Sensitivity (%)\", sensitivity*100))\nprint(\"{:<50} {:<20}\".format(\"Specificity (%)\", specificity*100))\nprint(\"{:<50} {:<20}\".format(\"F1 Score (%)\", f1_score*100))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:55:15.369791Z","iopub.execute_input":"2021-12-17T14:55:15.370062Z","iopub.status.idle":"2021-12-17T14:55:22.854179Z","shell.execute_reply.started":"2021-12-17T14:55:15.370027Z","shell.execute_reply":"2021-12-17T14:55:22.853096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy = int((sum(val_sum_true)+sum(val_sum_true_neg))/(sum(val_sum_true)+sum(val_sum_true_neg+sum(val_sum)+sum(val_sum_false_neg))))\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:55:22.855938Z","iopub.execute_input":"2021-12-17T14:55:22.856559Z","iopub.status.idle":"2021-12-17T14:55:22.862219Z","shell.execute_reply.started":"2021-12-17T14:55:22.856503Z","shell.execute_reply":"2021-12-17T14:55:22.861197Z"},"trusted":true},"execution_count":null,"outputs":[]}]}